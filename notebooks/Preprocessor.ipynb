{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing as mp\n",
    "from functools import reduce, partial\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from typing import List, Dict, NoReturn, Any, Callable, Union, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_indexed_df(df1: pd.core.frame.DataFrame, columname: str) -> pd.core.frame.DataFrame:\n",
    "    \"\"\" \n",
    "        Cast into a time-indexed dataframe.\n",
    "        df1 paramater should have a column containing datetime-like data,\n",
    "        which contains entries of type pandas._libs.tslibs.timestamps.Timestamp\n",
    "        or a string containing a compatible datetime (i.e. pd.to_datetime)\n",
    "    \"\"\"\n",
    "    \n",
    "    _tmp = df1.copy()\n",
    "    \n",
    "    pool = mp.Pool()\n",
    "    _tmp[columname] = pool.map(pd.to_datetime, _tmp[columname])\n",
    "    pool.close()\n",
    "    pool.terminate()\n",
    "    \n",
    "    _tmp.index = _tmp[columname]\n",
    "    _tmp.drop(columname, axis=1, inplace=True)\n",
    "    _tmp = _tmp.sort_index()\n",
    "    \n",
    "    return _tmp\n",
    "##\n",
    "\n",
    "def dist_plot(series: pd.core.series.Series, dropna: bool = True) -> NoReturn:\n",
    "    \"\"\"\n",
    "        Given a pandas Series, generate a descriptive visualisation \n",
    "        with a boxplot and a histogram with a kde.\n",
    "        By default, this function drops `nan` values. If you desire to\n",
    "        handle them differently, you should do so beforehand and/or\n",
    "        specify dropna=False.\n",
    "    \"\"\"\n",
    "    \n",
    "    if dropna:\n",
    "        series = series.dropna()\n",
    "    \n",
    "    f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.25, .75)})\n",
    "    sns.boxplot(series, ax=ax_box)\n",
    "    sns.stripplot(series, color=\"orange\", jitter=0.2, size=2.5, ax=ax_box)\n",
    "    sns.distplot(series, ax=ax_hist, kde=True)\n",
    "    ax_box.set(xlabel='')\n",
    "##\n",
    "\n",
    "\n",
    "def merge_on_duplicate_idx(\n",
    "    df: pd.core.frame.DataFrame, \n",
    "    mask: Any = np.nan,\n",
    "    verbose: bool = False\n",
    ") -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    y = df.copy()\n",
    "    y = y.mask( y == mask ).groupby(level=0).first()\n",
    "    \n",
    "    if verbose:\n",
    "        original_rows = df.shape[0]\n",
    "        duplicate_idx = df[df.index.duplicated(keep=False)].index\n",
    "        duplicate_rows = df.loc[duplicate_idx].shape[0]\n",
    "        new_rows = y.shape[0]\n",
    "        print(f\" Total rows on source dataframe :\\t{original_rows}\")\n",
    "        print(f\" Duplicate indices :\\t\\t\\t{duplicate_idx.shape[0]}\")\n",
    "        print(f\" Total duplicate rows :\\t\\t\\t{duplicate_rows}\")\n",
    "        print(f\" Rows on pruned dataframe :\\t\\t{new_rows}\")\n",
    "    \n",
    "    return y\n",
    "##\n",
    "\n",
    "def comparative_hba1c_plot(\n",
    "    df: pd.core.frame.DataFrame,\n",
    "    colum_name: str = \"Sensor Glucose (mg/dL)\",\n",
    "    hba1c: Callable = lambda x: (x + 105) / 36.5,\n",
    "    windows: Dict[str,int] = {\n",
    "        \"weekly\": 7,\n",
    "        \"monthly\": 30\n",
    "    }\n",
    ") -> NoReturn:\n",
    "    \"\"\"\n",
    "        \n",
    "    \"\"\"\n",
    "    df.groupby(df.index.dayofyear)[colum_name].mean().apply(hba1c).plot(**{\"label\":\"daily\"})\n",
    "    for key, value in windows.items():\n",
    "        df.groupby(df.index.dayofyear)[colum_name].mean().rolling(value).mean().apply(hba1c).plot(**{\"label\":key})\n",
    "    mean_hba1c = hba1c(df[colum_name].mean()) \n",
    "    plt.axhline(mean_hba1c, **{\"label\": f\"mean = {round(mean_hba1c,1)}\", \"c\": \"blue\"})\n",
    "    plt.legend()\n",
    "##\n",
    "\n",
    "def probability_estimate(\n",
    "    data: pd.core.series.Series, \n",
    "    start: float, \n",
    "    end: float, \n",
    "    N: int = 250,\n",
    "    percentage: bool = False,\n",
    "    show_plots: bool = False\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "        buggy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Plot the data using a normalized histogram\n",
    "    dev = copy.deepcopy(data)\n",
    "    dev = dev.dropna().apply(int)\n",
    "    \n",
    "    x = np.linspace(dev.min(), min(data), max(data))[:, np.newaxis]\n",
    "\n",
    "    # Do kernel density estimation\n",
    "    kd = KernelDensity(kernel='gaussian', bandwidth=0.85).fit(np.array(dev).reshape(-1, 1))\n",
    "\n",
    "    # Plot the estimated densty\n",
    "    kd_vals = np.exp(kd.score_samples(x))\n",
    "\n",
    "    # Show the plots\n",
    "    if show_plots:\n",
    "        plt.plot(x, kd_vals)\n",
    "        plt.hist(dev, 50, normed=True)\n",
    "        plt.xlabel('Concentration mg/dl')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title('Probability Density Esimation')\n",
    "        plt.show()\n",
    "\n",
    "    #probability = integrate(lambda x: np.exp(kd.score_samples(x.reshape(-1, 1))), start, end)[0]\n",
    "    \n",
    "    # Integration :\n",
    "    step = (end - start) / (N - 1)  # Step size\n",
    "    x = np.linspace(start, end, N)[:, np.newaxis]  # Generate values in the range\n",
    "    kd_vals = np.exp(kd.score_samples(x))  # Get PDF values for each x\n",
    "    probability = np.sum(kd_vals * step)  # Approximate the integral of the PDF\n",
    "    \n",
    "    if percentage:\n",
    "        probability *= 100\n",
    "    \n",
    "    return probability\n",
    "##\n",
    "\n",
    "def hybrid_interpolator(\n",
    "    data: pd.core.series.Series,\n",
    "    mean: float = None,\n",
    "    limit: float = None,\n",
    "    methods: List[str] = ['linear', 'spline'], \n",
    "    weights: List[float] = [0.65, 0.35],\n",
    "    direction: str = 'forward',\n",
    "    order: int = 2\n",
    ") -> pd.core.series.Series:\n",
    "    \"\"\"\n",
    "    Return a pandas.core.series.Series instance resulting of the weighted average\n",
    "    of two interpolation methods.\n",
    "    \n",
    "    Model:\n",
    "        φ = β1*method1 + β2*method2\n",
    "        \n",
    "    Default:\n",
    "        β1, β2 = 0.6, 0.4\n",
    "        method1, method2 = linear, spline\n",
    "    \n",
    "    Weights are meant to be numbers from the interval (0, 1)\n",
    "    which add up to one, to keep the weighted sum consistent.\n",
    "    \n",
    "    limit_direction : {‘forward’, ‘backward’, ‘both’}, default ‘forward’\n",
    "    If limit is specified, consecutive NaNs will be filled in this direction.\n",
    "    \n",
    "    If the predicted φ_i value is outside of the the interval\n",
    "    ( (mean - limit), (mean + limit) )\n",
    "    it will be replaced by the linear interpolation approximation.\n",
    "    \n",
    "    If not set, mean and limit will default to:\n",
    "        mean = data.mean()\n",
    "        limit = 2 * data.std()\n",
    "    \n",
    "    This function should have support for keyword arguments, but is yet to be implemented.\n",
    "    \"\"\"\n",
    "    predictions: List[float] = [] \n",
    "    \n",
    "    if not np.isclose(sum(weight for weight in weights), 1):\n",
    "        raise Exception('Sum of weights must be equal to one!')\n",
    "    \n",
    "    for met in methods:\n",
    "        if (met == 'spline') or (met == 'polynomial'):\n",
    "            predictions.append(data.interpolate(method=met, order=order, limit_direction=direction))\n",
    "        else:\n",
    "            predictions.append(data.interpolate(method=met, limit_direction=direction))\n",
    "\n",
    "    linear: pd.core.series.Series = predictions[0]\n",
    "    spline: pd.core.series.Series = predictions[1]\n",
    "    hybrid: pd.core.series.Series = weights[0]*predictions[0] + weights[1]*predictions[1]\n",
    "    \n",
    "    corrected: pd.core.series.Series = copy.deepcopy(hybrid) \n",
    "    \n",
    "    if not mean:\n",
    "        mean = data.mean()\n",
    "    if not limit:\n",
    "        limit = 2 * data.std()\n",
    "    \n",
    "    for idx, val in zip(hybrid[ np.isnan(data) ].index, hybrid[ np.isnan(data) ]):\n",
    "        if (val > mean + limit) or (val < mean - limit):\n",
    "            corrected[idx] = linear[idx]\n",
    "    \n",
    "    #df = copy.deepcopy(interpolated)\n",
    "    #print(df.isnull().astype(int).groupby(df.notnull().astype(int).cumsum()).sum())\n",
    "    \n",
    "    return corrected\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_csv_files = lambda loc: [os.path.join(loc, x) for x in os.listdir(loc) if x[-4:] == \".csv\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/apr/CareLink-Export-15-apr-2020-1-month.csv',\n",
       " 'data/apr/CareLink-Export-15-apr-2020-15-days.csv',\n",
       " 'data/apr/CareLink-Export-15-apr-2020-3-month.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = get_csv_files(\"data/apr/\")\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/diab/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (3,8,9,10,13,14,16,17,18,32,34,37,39,41,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "the_file = files[-1]\n",
    "x = pd.read_csv(the_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\"DateTime\"] =  x[\"Date\"] + \" \" + x[\"Time\"]\n",
    "x.drop([\"Date\", \"Time\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total rows on source dataframe :\t34185\n",
      " Duplicate indices :\t\t\t11127\n",
      " Total duplicate rows :\t\t\t32865\n",
      " Rows on pruned dataframe :\t\t27374\n"
     ]
    }
   ],
   "source": [
    "y = time_indexed_df(x, 'DateTime')\n",
    "y.drop(\"Index\", axis=1, inplace=True)\n",
    "\n",
    "# experimental  \n",
    "y.index = y.index.map(lambda t: t.replace(second=0))\n",
    "# end experimental\n",
    "\n",
    "y = merge_on_duplicate_idx(y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful having an hour column, for groupby opperations :\n",
    "y['hour'] = y.index.hour\n",
    "# Deltas within valuable intervals : \n",
    "for i in [10, 20, 30]: \n",
    "    y[f'd{i}'] = y['Sensor Glucose (mg/dL)'].diff(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code snippet generates *delta* columns. These are, however, suboptimal as logging seems to be inconsistent. This may be fixed via interpolation, but precaution is mandatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following attemps to *map* the hours and minutes from the datetime index to a [parametric circle](https://mathopenref.com/coordparamcircle.html). You might ask : **Why?**\n",
    "\n",
    "The insulin pump is configured to adjust the hourly basal dose of insulin. In general, a healthy pancreas would constantly secrete insulin responding to subtle variations in glycaemia. When there is a carb intake, i.e. a the person eats something such as bread, fruit, etc. glycaemia rises and this augmentation frees insulin to the bloodstream.\n",
    "\n",
    "Insulin sensibility varies throughout the day. The previously mentioned pump configuration has scheduled basal doses and **insulin-glycaemia-drop ratio, a.k.a insulin sensitivity** and **insulin-carbohidrate absorption ratio, alias carb ratio**.\n",
    "\n",
    "To better represent this periodicity, I've decided to create this two new periodic variables as the sine and cosine of the hour and minute of the day. This enables the expression of the periodicity of physiological phenomena, i.e. today's midnight is closer to tomorrow's morning than it is to the same day's morning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1439\n",
    "min_res_t_series = pd.Series(y.hour*60 + y.index.minute)\n",
    "y['x(t)'] = min_res_t_series.apply(lambda x: np.cos(2*np.pi*(x) / T))\n",
    "y['y(t)'] = min_res_t_series.apply(lambda x: np.sin(2*np.pi*(x) / T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.to_csv(\n",
    "    os.path.join(\n",
    "        \"preprocessed\", \n",
    "        os.path.split(the_file)[-1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
